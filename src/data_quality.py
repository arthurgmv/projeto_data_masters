from pyspark.sql import DataFrame
from pyspark.sql.functions import col
import logging

class DataQuality:
    def __init__(self, spark):
        self.spark = spark
        # Configura um logger simples para mostrar mensagens no terminal
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - [DATA QUALITY] - %(message)s', datefmt='%H:%M:%S')
        self.logger = logging.getLogger("DataQuality")

    def check_nulls(self, df: DataFrame, columns: list):
        """Verifica se hÃ¡ valores nulos em colunas crÃ­ticas"""
        self.logger.info(f"ğŸ” Verificando Nulos nas colunas: {columns}")
        
        for c in columns:
            null_count = df.filter(col(c).isNull() | (col(c) == "")).count()
            if null_count > 0:
                self.logger.warning(f"âš ï¸  ALERTA: Coluna '{c}' tem {null_count} registros nulos/vazios!")
            else:
                self.logger.info(f"âœ… Coluna '{c}' estÃ¡ Ã­ntegra (0 nulos).")
    
    def check_positive_values(self, df: DataFrame, columns: list):
        """Garante que valores numÃ©ricos sejam positivos (ex: Vendas)"""
        self.logger.info(f"ğŸ” Verificando valores negativos: {columns}")
        
        for c in columns:
            negative_count = df.filter(col(c) < 0).count()
            if negative_count > 0:
                self.logger.error(f"ğŸš¨ ERRO CRÃTICO: Coluna '{c}' possui {negative_count} valores negativos!")
            else:
                self.logger.info(f"âœ… Coluna '{c}' contÃ©m apenas valores positivos.")

    def count_rows(self, df: DataFrame, stage_name: str):
        """MÃ©trica de volumetria (Observabilidade)"""
        count = df.count()
        self.logger.info(f"ğŸ“Š [OBSERVABILITY] Total de linhas em {stage_name}: {count}")
        return count